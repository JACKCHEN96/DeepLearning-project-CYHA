{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Columbia University\n",
    "### ECBM E4040 Neural Networks and Deep Learning. Fall 2019."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 1, Task 4: Questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1 \n",
    "Cross entropy is a metric that measures the \"distance\" between two distributions, why can it be used in calculating the loss of softmax classifier? \n",
    "\n",
    "   Your answer: **[We can see that the functions of Cross entropy ( E=-∑yilog(pj) ) and Softmax loss ( L=-∑yilog(sj) ) are the same. \n",
    "   When pj=sj, they have the same output. In other words, if the input p is the output of softmax, cross entropy equals softmax loss.]**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2 \n",
    "Please first describe the difference between multi-class and binary logistic regression; then describe another possible way to derive a multi-class logistic regression classifier from a binary one; finally, illustrate how they work in a deep learning classification model.\n",
    "\n",
    "   Your answer: **[The binary logistic regression has only two classes, while the multi-class logistic regression has more than two classes.\n",
    "    In binary logistic regression, we use sigmoid function. In multi-class logistic regression, we can change the loss function to softmax function.\n",
    "     What's more, the multi-class one can also be done through one-vs-all scheme in which for each a binary logistic regression is done (giving 0/1).\n",
    "     For example, given a picture of a cat, four different recognizers might be trained.(1. Is it a apple? No. 2. Is it a wolf? No. 3 Is it a dog? No. 4. Is it a cat? Yes!)\n",
    "     Thus, one-vs-all provides a way to leverage binary classification. The approach is reasonable when the class number is small, but becomes inefficient as the class number rises.]**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3\n",
    "Why is the ReLU activation function used the most often in neural networks for computer vision?\n",
    "\n",
    "   Your answer: **[The ReLU function is f(x)=max(0,x). Usually this is applied element-wise to the output of some other function, such as a matrix-vector product.\n",
    "    ReLUs improve neural networks by speeding up training. The gradient computation is very simple, and the computational step of a ReLU is easy.\n",
    "     Gradients of logistic and hyperbolic tangent networks are smaller than the positive portion of the ReLU.]**\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 4\n",
    "**Cross validation** is a technique used to prove the generalization ability of a model and can help you find a robust set of hyperparameters. Please describe the implementation details of **k-fold cross validation**.\n",
    "\n",
    "   Your answer: **[Cross-validation is primarily used in applied machine learning to estimate the skill of a machine learning model on unseen data.\n",
    "    The procedure has a single parameter called k that refers to the number of groups that a given data sample is to split into.\n",
    "     The implementation is as follows:\n",
    "     1. Shuffle the dataset randomly.\n",
    "     2. Split the dataset into k groups.\n",
    "     3. For each unique group:\n",
    "        1. Take the group as a hold out or test data set.\n",
    "        2. Take the remaining groups as a training data set.\n",
    "        3. Fit a model on the training set and evaluate it on the test set.\n",
    "        4. Retain the evaluation score and discard the model.\n",
    "     4. Summarize the skill of teh model using the sample of model evaluation scores.]**\n",
    "   \n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 5\n",
    "Describe your best model in the implementation of the two-layer neural network. Describe your starting point, how you tuned  hyperparameters, which stategies you used to improve the network, show the results of intermediate and the final steps.\n",
    "\n",
    "   Your answer: **[The accuracy of my best model is 0.5091. (input_dim=3072, hidden_dim=150, num_classes=10, reg=1e-4, weight_scale=3e-3, num_epoch = 15, batch_size = 520, lr = 1e-3)\n",
    "   My starting point is the same as the default parameters. \n",
    "   First, I increase num_epoch to a high level, around 25. I found the accuracy increased, but there was overfitting. So I set num_epoch=15. \n",
    "   Next, I increased the hidden dimension, the accuracy certainly increased as I guessed. \n",
    "   At last, I just tuned other hyperparameters a little bit. For example, I set lr=3e-3. The accuracy increased a little and it achieved 50%.]**\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 6\n",
    "(Optional, this question is included in the 10 points bonus) In tSNE, describe the motivation of tuning the parameter and discuss the difference in results you see.\n",
    "    \n",
    "   Your answer: **[I just increase the low dim from 2 to 5, and increase the perplexity from 20 to 50. Then do the visualization, and the scatter plot becomes more concentrated at some points]**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}