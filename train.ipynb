{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from datetime import datetime\n",
    "import time\n",
    "import numpy as np\n",
    "import h5py\n",
    "import random\n",
    "from PIL import Image\n",
    "import tensorflow as tf\n",
    "import json\n",
    "from model import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_batch(path_to_tfrecords_file, num_examples, batch_size, shuffled):\n",
    "    assert tf.gfile.Exists(path_to_tfrecords_file), '%s not found' % path_to_tfrecords_file\n",
    "\n",
    "    filename_queue = tf.train.string_input_producer([path_to_tfrecords_file], num_epochs=None)\n",
    "    #image, length, digits = read_and_decode(filename_queue)\n",
    "\n",
    "    reader = tf.TFRecordReader()\n",
    "    _, serialized_example = reader.read(filename_queue)\n",
    "    features = tf.parse_single_example(\n",
    "            serialized_example,\n",
    "            features={\n",
    "                'image': tf.FixedLenFeature([], tf.string),\n",
    "                'length': tf.FixedLenFeature([], tf.int64),\n",
    "                'digits': tf.FixedLenFeature([5], tf.int64)\n",
    "            })\n",
    "\n",
    "    image = tf.decode_raw(features['image'], tf.uint8)\n",
    "    image = tf.image.convert_image_dtype(image, dtype=tf.float32)\n",
    "    image = tf.multiply(tf.subtract(image, 0.5), 2)\n",
    "    image = tf.reshape(image, [64, 64, 3])\n",
    "    image = tf.random_crop(image, [54, 54, 3])\n",
    "    \n",
    "    length = tf.cast(features['length'], tf.int32)\n",
    "    digits = tf.cast(features['digits'], tf.int32)\n",
    "    \n",
    "    \n",
    "    \n",
    "    min_queue_examples = int(0.4 * num_examples)\n",
    "\n",
    "    image_batch, length_batch, digits_batch = tf.train.batch([image, length, digits],\n",
    "                                                                     batch_size=batch_size,\n",
    "                                                                     num_threads=2,\n",
    "                                                                     capacity=min_queue_examples + 3 * batch_size)\n",
    "    return image_batch, length_batch, digits_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_summary(path_to_eval_log_dir):\n",
    "    summary_writer = tf.summary.FileWriter(path_to_eval_log_dir)\n",
    "    return summary_writer\n",
    "\n",
    "def evaluate(writer, path_to_checkpoint, path_to_tfrecords_file, num_examples, global_step):\n",
    "    batch_size = 128\n",
    "    num_batches = num_examples // batch_size\n",
    "    needs_include_length = False\n",
    "\n",
    "    with tf.Graph().as_default():\n",
    "        image_batch, length_batch, digits_batch = build_batch(path_to_tfrecords_file,\n",
    "                                                                         num_examples=num_examples,\n",
    "                                                                         batch_size=batch_size,\n",
    "                                                                         shuffled=False)\n",
    "        length_logits, digits_logits = Model.inference(image_batch, drop_rate=0.0)\n",
    "        length_predictions = tf.argmax(length_logits, axis=1)\n",
    "        digits_predictions = tf.argmax(digits_logits, axis=2)\n",
    "\n",
    "\n",
    "        labels = digits_batch\n",
    "        predictions = digits_predictions\n",
    "\n",
    "        labels_string = tf.reduce_join(tf.as_string(labels), axis=1)\n",
    "        predictions_string = tf.reduce_join(tf.as_string(predictions), axis=1)\n",
    "\n",
    "        accuracy, update_accuracy = tf.metrics.accuracy(\n",
    "        labels=labels_string,\n",
    "        predictions=predictions_string\n",
    "            )\n",
    "\n",
    "        tf.summary.image('image', image_batch)\n",
    "        tf.summary.scalar('accuracy', accuracy)\n",
    "        tf.summary.histogram('variables',\n",
    "                                 tf.concat([tf.reshape(var, [-1]) for var in tf.trainable_variables()], axis=0))\n",
    "        summary = tf.summary.merge_all()\n",
    "\n",
    "        with tf.Session() as sess:\n",
    "            sess.run([tf.global_variables_initializer(), tf.local_variables_initializer()])\n",
    "            coord = tf.train.Coordinator()\n",
    "            threads = tf.train.start_queue_runners(sess=sess, coord=coord)\n",
    "\n",
    "            restorer = tf.train.Saver()\n",
    "            restorer.restore(sess, path_to_checkpoint)\n",
    "\n",
    "            for _ in range(num_batches):\n",
    "                sess.run(update_accuracy)\n",
    "\n",
    "            accuracy_val, summary_val = sess.run([accuracy, summary])\n",
    "            writer.add_summary(summary_val, global_step=global_step)\n",
    "\n",
    "            coord.request_stop()\n",
    "            coord.join(threads)\n",
    "\n",
    "    return accuracy_val\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Build the training process\n",
    "\n",
    "def train(path_to_train_tfrecords_file, num_train_examples, path_to_val_tfrecords_file, num_val_examples,\n",
    "           path_to_train_log_dir, training_options):\n",
    "    batch_size = training_options['batch_size']\n",
    "    initial_patience = training_options['patience']\n",
    "    num_steps_to_show_loss = 10\n",
    "    num_steps_to_check = 100\n",
    "\n",
    "    with tf.Graph().as_default():\n",
    "        image_batch, length_batch, digits_batch = build_batch(path_to_train_tfrecords_file,\n",
    "                                                                     num_examples=num_train_examples,\n",
    "                                                                     batch_size=batch_size,\n",
    "                                                                     shuffled=True)\n",
    "        \n",
    "        length_logtis, digits_logits = Model.inference(image_batch, drop_rate=0.2)\n",
    "        loss = Model.loss(length_logtis, digits_logits, length_batch, digits_batch)\n",
    "        \n",
    "        global_step = tf.Variable(0, name='global_step', trainable=False)\n",
    "        learning_rate = tf.train.exponential_decay(training_options['learning_rate'], global_step=global_step,\n",
    "                                                   decay_steps=training_options['decay_steps'], decay_rate=training_options['decay_rate'], staircase=True)\n",
    "        optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "        train_op = optimizer.minimize(loss, global_step=global_step)\n",
    "\n",
    "        tf.summary.image('image', image_batch)\n",
    "        tf.summary.scalar('loss', loss)\n",
    "        tf.summary.scalar('learning_rate', learning_rate)\n",
    "        summary = tf.summary.merge_all()\n",
    "\n",
    "        with tf.Session() as sess:\n",
    "            summary_writer = tf.summary.FileWriter(path_to_train_log_dir, sess.graph)\n",
    "            evaluator = write_summary(os.path.join(path_to_train_log_dir, 'eval/val'))\n",
    "            #evaluator = Evaluator(os.path.join(path_to_train_log_dir, 'eval/val'))\n",
    "    \n",
    "            #tf.global_variables_initializer()\n",
    "            sess.run(tf.global_variables_initializer())\n",
    "            coord = tf.train.Coordinator()\n",
    "            threads = tf.train.start_queue_runners(sess=sess, coord=coord)\n",
    "\n",
    "            saver = tf.train.Saver()\n",
    "\n",
    "            print ('Start training')\n",
    "            patience = initial_patience\n",
    "            best_accuracy = 0.0\n",
    "            duration = 0.0\n",
    "            epoch = 32000\n",
    "            k=0\n",
    "            for i in range(epoch):\n",
    "                k+=1\n",
    "                start_time = time.time()\n",
    "                _, loss_val, summary_val, global_step_val, learning_rate_val = sess.run([train_op, loss, summary, global_step, learning_rate])\n",
    "                duration += time.time() - start_time\n",
    "\n",
    "                if global_step_val % num_steps_to_show_loss == 0:\n",
    "                    examples_per_sec = batch_size * num_steps_to_show_loss / duration\n",
    "                    duration = 0.0\n",
    "                    print ('%s: step %d, loss = %f ' % (\n",
    "                        datetime.now(), global_step_val, loss_val))\n",
    "\n",
    "                if global_step_val % num_steps_to_check != 0:\n",
    "                    continue\n",
    "\n",
    "                summary_writer.add_summary(summary_val, global_step=global_step_val)\n",
    "\n",
    "\n",
    "                path_to_latest_checkpoint_file = saver.save(sess, os.path.join(path_to_train_log_dir, 'latest.ckpt'))\n",
    "                accuracy = evaluate(evaluator, path_to_latest_checkpoint_file, path_to_val_tfrecords_file,\n",
    "                                              num_val_examples,\n",
    "                                              global_step_val)\n",
    "                print ('Validation accuracy is= %f, best accuracy %f' % (accuracy, best_accuracy))\n",
    "\n",
    "                if accuracy > best_accuracy:\n",
    "                    path_to_checkpoint_file = saver.save(sess, os.path.join(path_to_train_log_dir, 'model.ckpt'),\n",
    "                                                         global_step=global_step_val)\n",
    "                    print ('Save file to: %s' % path_to_checkpoint_file)\n",
    "                    patience = initial_patience\n",
    "                    best_accuracy = accuracy\n",
    "                else:\n",
    "                    patience -= 1\n",
    "\n",
    "\n",
    "                if k>=epoch:\n",
    "                    break\n",
    "\n",
    "            coord.request_stop()\n",
    "            coord.join(threads)\n",
    "\n",
    "            print ('Training progess is finished')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-2-e970b1552c5c>:4: string_input_producer (from tensorflow.python.training.input) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Queue-based input pipelines have been replaced by `tf.data`. Use `tf.data.Dataset.from_tensor_slices(string_tensor).shuffle(tf.shape(input_tensor, out_type=tf.int64)[0]).repeat(num_epochs)`. If `shuffle=False`, omit the `.shuffle(...)`.\n",
      "WARNING:tensorflow:From /home/ecbm4040/miniconda3/envs/envTF113/lib/python3.6/site-packages/tensorflow/python/training/input.py:278: input_producer (from tensorflow.python.training.input) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Queue-based input pipelines have been replaced by `tf.data`. Use `tf.data.Dataset.from_tensor_slices(input_tensor).shuffle(tf.shape(input_tensor, out_type=tf.int64)[0]).repeat(num_epochs)`. If `shuffle=False`, omit the `.shuffle(...)`.\n",
      "WARNING:tensorflow:From /home/ecbm4040/miniconda3/envs/envTF113/lib/python3.6/site-packages/tensorflow/python/training/input.py:190: limit_epochs (from tensorflow.python.training.input) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Queue-based input pipelines have been replaced by `tf.data`. Use `tf.data.Dataset.from_tensors(tensor).repeat(num_epochs)`.\n",
      "WARNING:tensorflow:From /home/ecbm4040/miniconda3/envs/envTF113/lib/python3.6/site-packages/tensorflow/python/training/input.py:199: QueueRunner.__init__ (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "To construct input pipelines, use the `tf.data` module.\n",
      "WARNING:tensorflow:From /home/ecbm4040/miniconda3/envs/envTF113/lib/python3.6/site-packages/tensorflow/python/training/input.py:199: add_queue_runner (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "To construct input pipelines, use the `tf.data` module.\n",
      "WARNING:tensorflow:From /home/ecbm4040/miniconda3/envs/envTF113/lib/python3.6/site-packages/tensorflow/python/training/input.py:202: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "WARNING:tensorflow:From <ipython-input-2-e970b1552c5c>:7: TFRecordReader.__init__ (from tensorflow.python.ops.io_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Queue-based input pipelines have been replaced by `tf.data`. Use `tf.data.TFRecordDataset`.\n",
      "WARNING:tensorflow:From /home/ecbm4040/miniconda3/envs/envTF113/lib/python3.6/site-packages/tensorflow/python/ops/control_flow_ops.py:3632: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From <ipython-input-2-e970b1552c5c>:33: batch (from tensorflow.python.training.input) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Queue-based input pipelines have been replaced by `tf.data`. Use `tf.data.Dataset.batch(batch_size)` (or `padded_batch(...)` if `dynamic_pad=True`).\n",
      "WARNING:tensorflow:From /home/ecbm4040/ECBM-E4040-Final-master-1/ECBM-E4040-Final-master/model.py:9: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.conv2d instead.\n",
      "WARNING:tensorflow:From /home/ecbm4040/ECBM-E4040-Final-master-1/ECBM-E4040-Final-master/model.py:10: batch_normalization (from tensorflow.python.layers.normalization) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.batch_normalization instead.\n",
      "WARNING:tensorflow:From /home/ecbm4040/ECBM-E4040-Final-master-1/ECBM-E4040-Final-master/model.py:12: max_pooling2d (from tensorflow.python.layers.pooling) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.max_pooling2d instead.\n",
      "WARNING:tensorflow:From /home/ecbm4040/ECBM-E4040-Final-master-1/ECBM-E4040-Final-master/model.py:13: dropout (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.dropout instead.\n",
      "WARNING:tensorflow:From /home/ecbm4040/ECBM-E4040-Final-master-1/ECBM-E4040-Final-master/model.py:75: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.dense instead.\n",
      "WARNING:tensorflow:From <ipython-input-4-bec2aae80163>:38: start_queue_runners (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "To construct input pipelines, use the `tf.data` module.\n",
      "Start training\n",
      "2019-12-06 16:13:26.489701: step 10, loss = 12.269635 \n",
      "2019-12-06 16:13:27.266948: step 20, loss = 11.954708 \n",
      "2019-12-06 16:13:28.062674: step 30, loss = 7.894696 \n",
      "2019-12-06 16:13:28.874975: step 40, loss = 7.695097 \n",
      "2019-12-06 16:13:29.687583: step 50, loss = 6.477096 \n",
      "2019-12-06 16:13:30.481715: step 60, loss = 6.951155 \n",
      "2019-12-06 16:13:31.260338: step 70, loss = 7.605587 \n",
      "2019-12-06 16:13:32.056756: step 80, loss = 7.371504 \n",
      "2019-12-06 16:13:32.869666: step 90, loss = 6.744147 \n",
      "2019-12-06 16:13:33.664958: step 100, loss = 7.984851 \n",
      "WARNING:tensorflow:From /home/ecbm4040/miniconda3/envs/envTF113/lib/python3.6/site-packages/tensorflow/python/training/saver.py:1266: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use standard file APIs to check for files with this prefix.\n",
      "INFO:tensorflow:Restoring parameters from logs5/train/latest.ckpt\n",
      "Validation accuracy is= 0.020733, best accuracy 0.000000\n",
      "Save file to: logs5/train/model.ckpt-100\n",
      "2019-12-06 16:13:45.859755: step 110, loss = 6.418836 \n",
      "2019-12-06 16:13:46.658266: step 120, loss = 7.050148 \n",
      "2019-12-06 16:13:47.448960: step 130, loss = 7.309947 \n",
      "2019-12-06 16:13:48.244814: step 140, loss = 7.061946 \n",
      "2019-12-06 16:13:49.055847: step 150, loss = 7.443737 \n",
      "2019-12-06 16:13:49.837105: step 160, loss = 6.489099 \n",
      "2019-12-06 16:13:50.619253: step 170, loss = 8.336694 \n",
      "2019-12-06 16:13:51.394595: step 180, loss = 6.628560 \n",
      "2019-12-06 16:13:52.165769: step 190, loss = 6.171857 \n",
      "2019-12-06 16:13:52.935169: step 200, loss = 7.343713 \n",
      "INFO:tensorflow:Restoring parameters from logs5/train/latest.ckpt\n",
      "Validation accuracy is= 0.018930, best accuracy 0.020733\n",
      "2019-12-06 16:14:04.991454: step 210, loss = 6.591098 \n",
      "2019-12-06 16:14:05.774110: step 220, loss = 7.302799 \n",
      "2019-12-06 16:14:06.569956: step 230, loss = 6.905340 \n",
      "2019-12-06 16:14:07.351756: step 240, loss = 7.317924 \n",
      "2019-12-06 16:14:08.149419: step 250, loss = 7.477624 \n",
      "2019-12-06 16:14:08.947587: step 260, loss = 7.007584 \n",
      "2019-12-06 16:14:09.748854: step 270, loss = 7.341481 \n",
      "2019-12-06 16:14:10.556741: step 280, loss = 6.504868 \n",
      "2019-12-06 16:14:11.392670: step 290, loss = 6.492020 \n",
      "2019-12-06 16:14:12.185346: step 300, loss = 7.481978 \n",
      "INFO:tensorflow:Restoring parameters from logs5/train/latest.ckpt\n",
      "Validation accuracy is= 0.018930, best accuracy 0.020733\n",
      "2019-12-06 16:14:21.476556: step 310, loss = 6.724839 \n",
      "2019-12-06 16:14:22.287877: step 320, loss = 6.715003 \n",
      "2019-12-06 16:14:23.089131: step 330, loss = 6.652013 \n",
      "2019-12-06 16:14:23.897054: step 340, loss = 7.217798 \n",
      "2019-12-06 16:14:24.691671: step 350, loss = 7.130971 \n",
      "2019-12-06 16:14:25.489731: step 360, loss = 6.884875 \n",
      "2019-12-06 16:14:26.321193: step 370, loss = 6.736772 \n",
      "2019-12-06 16:14:27.113029: step 380, loss = 6.652324 \n",
      "2019-12-06 16:14:27.920969: step 390, loss = 7.758391 \n",
      "2019-12-06 16:14:28.719995: step 400, loss = 7.651508 \n",
      "INFO:tensorflow:Restoring parameters from logs5/train/latest.ckpt\n",
      "Validation accuracy is= 0.018930, best accuracy 0.020733\n",
      "2019-12-06 16:14:38.016517: step 410, loss = 7.022032 \n",
      "2019-12-06 16:14:38.802891: step 420, loss = 7.000841 \n",
      "2019-12-06 16:14:39.575118: step 430, loss = 7.050903 \n",
      "2019-12-06 16:14:40.362350: step 440, loss = 7.123539 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-12-06 16:14:41.163990: step 450, loss = 6.307104 \n",
      "2019-12-06 16:14:41.962348: step 460, loss = 7.555735 \n",
      "2019-12-06 16:14:42.762343: step 470, loss = 7.478161 \n",
      "2019-12-06 16:14:43.542537: step 480, loss = 6.828054 \n",
      "2019-12-06 16:14:44.355928: step 490, loss = 5.943374 \n",
      "2019-12-06 16:14:45.141430: step 500, loss = 6.399864 \n",
      "INFO:tensorflow:Restoring parameters from logs5/train/latest.ckpt\n",
      "Validation accuracy is= 0.021334, best accuracy 0.020733\n",
      "Save file to: logs5/train/model.ckpt-500\n",
      "2019-12-06 16:14:54.359897: step 510, loss = 6.651124 \n",
      "2019-12-06 16:14:55.154351: step 520, loss = 7.364427 \n",
      "2019-12-06 16:14:55.943634: step 530, loss = 6.888393 \n",
      "2019-12-06 16:14:56.738379: step 540, loss = 7.181553 \n",
      "2019-12-06 16:14:57.529415: step 550, loss = 6.578374 \n",
      "2019-12-06 16:14:58.345881: step 560, loss = 6.763027 \n",
      "2019-12-06 16:14:59.127479: step 570, loss = 6.485048 \n",
      "2019-12-06 16:14:59.929805: step 580, loss = 6.581409 \n",
      "2019-12-06 16:15:00.710492: step 590, loss = 6.256154 \n",
      "2019-12-06 16:15:01.519591: step 600, loss = 6.630765 \n",
      "INFO:tensorflow:Restoring parameters from logs5/train/latest.ckpt\n",
      "Validation accuracy is= 0.020433, best accuracy 0.021334\n",
      "2019-12-06 16:15:10.410326: step 610, loss = 5.583905 \n",
      "2019-12-06 16:15:11.209445: step 620, loss = 7.019545 \n",
      "2019-12-06 16:15:12.013563: step 630, loss = 6.453707 \n",
      "2019-12-06 16:15:12.802068: step 640, loss = 6.948442 \n",
      "2019-12-06 16:15:13.604132: step 650, loss = 6.105889 \n",
      "2019-12-06 16:15:14.381592: step 660, loss = 6.535716 \n",
      "2019-12-06 16:15:15.165344: step 670, loss = 6.349156 \n",
      "2019-12-06 16:15:15.997324: step 680, loss = 6.000850 \n",
      "2019-12-06 16:15:16.816084: step 690, loss = 6.518225 \n",
      "2019-12-06 16:15:17.594023: step 700, loss = 5.844246 \n",
      "INFO:tensorflow:Restoring parameters from logs5/train/latest.ckpt\n",
      "Validation accuracy is= 0.023738, best accuracy 0.021334\n",
      "Save file to: logs5/train/model.ckpt-700\n",
      "2019-12-06 16:15:27.579471: step 710, loss = 7.129534 \n",
      "2019-12-06 16:15:28.378376: step 720, loss = 6.895782 \n",
      "2019-12-06 16:15:29.195799: step 730, loss = 6.502814 \n",
      "2019-12-06 16:15:29.998444: step 740, loss = 6.725830 \n",
      "2019-12-06 16:15:30.784901: step 750, loss = 6.154824 \n",
      "2019-12-06 16:15:31.555325: step 760, loss = 6.963813 \n",
      "2019-12-06 16:15:32.354996: step 770, loss = 5.641529 \n",
      "2019-12-06 16:15:33.145708: step 780, loss = 8.670944 \n",
      "2019-12-06 16:15:33.932356: step 790, loss = 6.644098 \n",
      "2019-12-06 16:15:34.707798: step 800, loss = 6.345541 \n",
      "INFO:tensorflow:Restoring parameters from logs5/train/latest.ckpt\n",
      "Validation accuracy is= 0.021635, best accuracy 0.023738\n",
      "2019-12-06 16:15:43.437063: step 810, loss = 6.418602 \n",
      "2019-12-06 16:15:44.240635: step 820, loss = 6.597013 \n",
      "2019-12-06 16:15:45.041772: step 830, loss = 5.700063 \n",
      "2019-12-06 16:15:45.831463: step 840, loss = 6.766825 \n",
      "2019-12-06 16:15:46.615552: step 850, loss = 6.033307 \n",
      "2019-12-06 16:15:47.384412: step 860, loss = 6.331169 \n",
      "2019-12-06 16:15:48.155005: step 870, loss = 6.088048 \n",
      "2019-12-06 16:15:48.949471: step 880, loss = 6.479167 \n",
      "2019-12-06 16:15:49.743439: step 890, loss = 7.350883 \n",
      "2019-12-06 16:15:50.532327: step 900, loss = 7.295014 \n",
      "INFO:tensorflow:Restoring parameters from logs5/train/latest.ckpt\n",
      "Validation accuracy is= 0.023137, best accuracy 0.023738\n",
      "2019-12-06 16:15:59.188263: step 910, loss = 6.610109 \n",
      "2019-12-06 16:15:59.972164: step 920, loss = 6.103705 \n",
      "2019-12-06 16:16:00.785913: step 930, loss = 5.933179 \n",
      "2019-12-06 16:16:01.573076: step 940, loss = 6.493474 \n",
      "2019-12-06 16:16:02.354180: step 950, loss = 8.706935 \n",
      "2019-12-06 16:16:03.147216: step 960, loss = 6.212171 \n",
      "2019-12-06 16:16:03.928121: step 970, loss = 6.584101 \n",
      "2019-12-06 16:16:04.714464: step 980, loss = 6.935502 \n",
      "2019-12-06 16:16:05.499429: step 990, loss = 6.013909 \n",
      "2019-12-06 16:16:06.288932: step 1000, loss = 6.424832 \n",
      "INFO:tensorflow:Restoring parameters from logs5/train/latest.ckpt\n",
      "Validation accuracy is= 0.023738, best accuracy 0.023738\n",
      "2019-12-06 16:16:15.995646: step 1010, loss = 6.794616 \n",
      "2019-12-06 16:16:16.783882: step 1020, loss = 5.832499 \n",
      "2019-12-06 16:16:17.579031: step 1030, loss = 5.995934 \n",
      "2019-12-06 16:16:18.375945: step 1040, loss = 6.266521 \n",
      "2019-12-06 16:16:19.184956: step 1050, loss = 6.159408 \n",
      "2019-12-06 16:16:19.985104: step 1060, loss = 7.141420 \n",
      "2019-12-06 16:16:20.769782: step 1070, loss = 6.985236 \n",
      "2019-12-06 16:16:21.575234: step 1080, loss = 6.253621 \n",
      "2019-12-06 16:16:22.371873: step 1090, loss = 6.659178 \n",
      "2019-12-06 16:16:23.166733: step 1100, loss = 6.095218 \n",
      "INFO:tensorflow:Restoring parameters from logs5/train/latest.ckpt\n",
      "Validation accuracy is= 0.027644, best accuracy 0.023738\n",
      "Save file to: logs5/train/model.ckpt-1100\n",
      "2019-12-06 16:16:32.202120: step 1110, loss = 5.963315 \n",
      "2019-12-06 16:16:32.993058: step 1120, loss = 6.951912 \n",
      "2019-12-06 16:16:33.791947: step 1130, loss = 7.750436 \n",
      "2019-12-06 16:16:34.579844: step 1140, loss = 6.244305 \n",
      "2019-12-06 16:16:35.371309: step 1150, loss = 6.332998 \n",
      "2019-12-06 16:16:36.153933: step 1160, loss = 6.809576 \n",
      "2019-12-06 16:16:36.960696: step 1170, loss = 6.020857 \n",
      "2019-12-06 16:16:37.771542: step 1180, loss = 6.658237 \n",
      "2019-12-06 16:16:38.557370: step 1190, loss = 6.286359 \n",
      "2019-12-06 16:16:39.353718: step 1200, loss = 5.616515 \n",
      "INFO:tensorflow:Restoring parameters from logs5/train/latest.ckpt\n",
      "Validation accuracy is= 0.030950, best accuracy 0.027644\n",
      "WARNING:tensorflow:From /home/ecbm4040/miniconda3/envs/envTF113/lib/python3.6/site-packages/tensorflow/python/training/saver.py:966: remove_checkpoint (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use standard file APIs to delete files with this prefix.\n",
      "Save file to: logs5/train/model.ckpt-1200\n",
      "2019-12-06 16:16:48.727812: step 1210, loss = 6.709252 \n",
      "2019-12-06 16:16:49.523361: step 1220, loss = 6.003513 \n",
      "2019-12-06 16:16:50.314911: step 1230, loss = 6.110487 \n",
      "2019-12-06 16:16:51.147111: step 1240, loss = 5.301068 \n",
      "2019-12-06 16:16:51.929822: step 1250, loss = 6.192640 \n",
      "2019-12-06 16:16:52.725394: step 1260, loss = 5.774840 \n",
      "2019-12-06 16:16:53.528332: step 1270, loss = 6.455042 \n",
      "2019-12-06 16:16:54.324463: step 1280, loss = 6.438216 \n",
      "2019-12-06 16:16:55.114472: step 1290, loss = 6.086748 \n",
      "2019-12-06 16:16:55.920372: step 1300, loss = 5.703514 \n",
      "INFO:tensorflow:Restoring parameters from logs5/train/latest.ckpt\n",
      "Validation accuracy is= 0.038762, best accuracy 0.030950\n",
      "Save file to: logs5/train/model.ckpt-1300\n",
      "2019-12-06 16:17:05.757421: step 1310, loss = 6.415649 \n",
      "2019-12-06 16:17:06.553533: step 1320, loss = 6.162174 \n",
      "2019-12-06 16:17:07.350582: step 1330, loss = 6.466881 \n",
      "2019-12-06 16:17:08.168963: step 1340, loss = 5.650603 \n",
      "2019-12-06 16:17:08.944491: step 1350, loss = 5.298911 \n",
      "2019-12-06 16:17:09.742688: step 1360, loss = 5.660931 \n",
      "2019-12-06 16:17:10.526799: step 1370, loss = 5.056035 \n",
      "2019-12-06 16:17:11.335137: step 1380, loss = 5.853781 \n",
      "2019-12-06 16:17:12.127177: step 1390, loss = 6.244047 \n",
      "2019-12-06 16:17:12.909273: step 1400, loss = 5.729223 \n",
      "INFO:tensorflow:Restoring parameters from logs5/train/latest.ckpt\n",
      "Validation accuracy is= 0.039663, best accuracy 0.038762\n",
      "Save file to: logs5/train/model.ckpt-1400\n",
      "2019-12-06 16:17:22.346255: step 1410, loss = 5.679379 \n",
      "2019-12-06 16:17:23.136210: step 1420, loss = 5.493755 \n",
      "2019-12-06 16:17:23.925059: step 1430, loss = 5.413899 \n",
      "2019-12-06 16:17:24.716163: step 1440, loss = 5.556993 \n",
      "2019-12-06 16:17:25.501641: step 1450, loss = 5.640398 \n",
      "2019-12-06 16:17:26.290048: step 1460, loss = 6.184503 \n",
      "2019-12-06 16:17:27.069110: step 1470, loss = 6.474167 \n",
      "2019-12-06 16:17:27.857879: step 1480, loss = 5.893163 \n",
      "2019-12-06 16:17:28.662792: step 1490, loss = 5.752269 \n",
      "2019-12-06 16:17:29.463208: step 1500, loss = 5.317395 \n",
      "INFO:tensorflow:Restoring parameters from logs5/train/latest.ckpt\n",
      "Validation accuracy is= 0.051382, best accuracy 0.039663\n",
      "Save file to: logs5/train/model.ckpt-1500\n",
      "2019-12-06 16:17:38.580563: step 1510, loss = 5.843815 \n",
      "2019-12-06 16:17:39.367099: step 1520, loss = 5.399678 \n",
      "2019-12-06 16:17:40.153962: step 1530, loss = 5.503467 \n",
      "2019-12-06 16:17:40.935583: step 1540, loss = 5.572937 \n",
      "2019-12-06 16:17:41.735081: step 1550, loss = 6.067220 \n",
      "2019-12-06 16:17:42.529331: step 1560, loss = 5.031445 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-12-06 16:17:43.328136: step 1570, loss = 5.956485 \n",
      "2019-12-06 16:17:44.144140: step 1580, loss = 4.951407 \n",
      "2019-12-06 16:17:44.929179: step 1590, loss = 6.115868 \n",
      "2019-12-06 16:17:45.722839: step 1600, loss = 5.920360 \n",
      "INFO:tensorflow:Restoring parameters from logs5/train/latest.ckpt\n",
      "Validation accuracy is= 0.061899, best accuracy 0.051382\n",
      "Save file to: logs5/train/model.ckpt-1600\n",
      "2019-12-06 16:17:55.462596: step 1610, loss = 5.659116 \n",
      "2019-12-06 16:17:56.256027: step 1620, loss = 5.834781 \n",
      "2019-12-06 16:17:57.079820: step 1630, loss = 5.406193 \n",
      "2019-12-06 16:17:57.896586: step 1640, loss = 5.594990 \n",
      "2019-12-06 16:17:58.700324: step 1650, loss = 5.438368 \n",
      "2019-12-06 16:17:59.499340: step 1660, loss = 5.703244 \n",
      "2019-12-06 16:18:00.303093: step 1670, loss = 4.950325 \n",
      "2019-12-06 16:18:01.137714: step 1680, loss = 5.337277 \n",
      "2019-12-06 16:18:01.931929: step 1690, loss = 4.257630 \n",
      "2019-12-06 16:18:02.743031: step 1700, loss = 5.614092 \n",
      "INFO:tensorflow:Restoring parameters from logs5/train/latest.ckpt\n",
      "Validation accuracy is= 0.074820, best accuracy 0.061899\n",
      "Save file to: logs5/train/model.ckpt-1700\n",
      "2019-12-06 16:18:11.769878: step 1710, loss = 5.283029 \n",
      "2019-12-06 16:18:12.550489: step 1720, loss = 5.294634 \n",
      "2019-12-06 16:18:13.341334: step 1730, loss = 7.314464 \n",
      "2019-12-06 16:18:14.126830: step 1740, loss = 5.114387 \n",
      "2019-12-06 16:18:14.915023: step 1750, loss = 4.933080 \n",
      "2019-12-06 16:18:15.690720: step 1760, loss = 4.840903 \n",
      "2019-12-06 16:18:16.478465: step 1770, loss = 4.982932 \n",
      "2019-12-06 16:18:17.269007: step 1780, loss = 5.494588 \n",
      "2019-12-06 16:18:18.044554: step 1790, loss = 5.554477 \n",
      "2019-12-06 16:18:18.839523: step 1800, loss = 5.286572 \n",
      "INFO:tensorflow:Restoring parameters from logs5/train/latest.ckpt\n",
      "Validation accuracy is= 0.097656, best accuracy 0.074820\n",
      "Save file to: logs5/train/model.ckpt-1800\n",
      "2019-12-06 16:18:28.119995: step 1810, loss = 5.530485 \n",
      "2019-12-06 16:18:28.922354: step 1820, loss = 5.569355 \n",
      "2019-12-06 16:18:29.735805: step 1830, loss = 5.147530 \n",
      "2019-12-06 16:18:30.521928: step 1840, loss = 5.432831 \n",
      "2019-12-06 16:18:31.316646: step 1850, loss = 4.900618 \n",
      "2019-12-06 16:18:32.115570: step 1860, loss = 4.742811 \n",
      "2019-12-06 16:18:32.907588: step 1870, loss = 5.326756 \n",
      "2019-12-06 16:18:33.715411: step 1880, loss = 5.489164 \n",
      "2019-12-06 16:18:34.521583: step 1890, loss = 5.110361 \n",
      "2019-12-06 16:18:35.338926: step 1900, loss = 4.366344 \n",
      "INFO:tensorflow:Restoring parameters from logs5/train/latest.ckpt\n",
      "Validation accuracy is= 0.097356, best accuracy 0.097656\n",
      "2019-12-06 16:18:44.854637: step 1910, loss = 6.477329 \n",
      "2019-12-06 16:18:45.625599: step 1920, loss = 4.062607 \n",
      "2019-12-06 16:18:46.413813: step 1930, loss = 5.150352 \n",
      "2019-12-06 16:18:47.221488: step 1940, loss = 5.630080 \n",
      "2019-12-06 16:18:47.983288: step 1950, loss = 4.667124 \n",
      "2019-12-06 16:18:48.761103: step 1960, loss = 5.380269 \n",
      "2019-12-06 16:18:49.573565: step 1970, loss = 5.247771 \n",
      "2019-12-06 16:18:50.364636: step 1980, loss = 4.032408 \n",
      "2019-12-06 16:18:51.190176: step 1990, loss = 4.938306 \n",
      "2019-12-06 16:18:51.982695: step 2000, loss = 5.107497 \n",
      "INFO:tensorflow:Restoring parameters from logs5/train/latest.ckpt\n",
      "Validation accuracy is= 0.128906, best accuracy 0.097656\n",
      "Save file to: logs5/train/model.ckpt-2000\n",
      "2019-12-06 16:19:01.012759: step 2010, loss = 5.366874 \n",
      "2019-12-06 16:19:01.798802: step 2020, loss = 4.859939 \n",
      "2019-12-06 16:19:02.563886: step 2030, loss = 4.847089 \n",
      "2019-12-06 16:19:03.355479: step 2040, loss = 4.693048 \n",
      "2019-12-06 16:19:04.130879: step 2050, loss = 5.122843 \n",
      "2019-12-06 16:19:04.917505: step 2060, loss = 4.590364 \n",
      "2019-12-06 16:19:05.693741: step 2070, loss = 5.828481 \n",
      "2019-12-06 16:19:06.474479: step 2080, loss = 4.531044 \n",
      "2019-12-06 16:19:07.241101: step 2090, loss = 6.117977 \n",
      "2019-12-06 16:19:08.009389: step 2100, loss = 5.017983 \n",
      "INFO:tensorflow:Restoring parameters from logs5/train/latest.ckpt\n",
      "Validation accuracy is= 0.146334, best accuracy 0.128906\n",
      "Save file to: logs5/train/model.ckpt-2100\n",
      "2019-12-06 16:19:16.846031: step 2110, loss = 5.383434 \n",
      "2019-12-06 16:19:17.638289: step 2120, loss = 4.855284 \n",
      "2019-12-06 16:19:18.438092: step 2130, loss = 5.579449 \n",
      "2019-12-06 16:19:19.243176: step 2140, loss = 5.079845 \n",
      "2019-12-06 16:19:20.063568: step 2150, loss = 4.904582 \n",
      "2019-12-06 16:19:20.839744: step 2160, loss = 4.682415 \n",
      "2019-12-06 16:19:21.626775: step 2170, loss = 5.844472 \n",
      "2019-12-06 16:19:22.420442: step 2180, loss = 5.371782 \n",
      "2019-12-06 16:19:23.218478: step 2190, loss = 4.363187 \n",
      "2019-12-06 16:19:24.032917: step 2200, loss = 4.983890 \n",
      "INFO:tensorflow:Restoring parameters from logs5/train/latest.ckpt\n",
      "Validation accuracy is= 0.146034, best accuracy 0.146334\n",
      "2019-12-06 16:19:33.154979: step 2210, loss = 4.824720 \n",
      "2019-12-06 16:19:33.956918: step 2220, loss = 4.765625 \n",
      "2019-12-06 16:19:34.749929: step 2230, loss = 4.846504 \n",
      "2019-12-06 16:19:35.540960: step 2240, loss = 4.224214 \n",
      "2019-12-06 16:19:36.351062: step 2250, loss = 4.084826 \n",
      "2019-12-06 16:19:37.133558: step 2260, loss = 3.904441 \n",
      "2019-12-06 16:19:37.933891: step 2270, loss = 4.033597 \n",
      "2019-12-06 16:19:38.727347: step 2280, loss = 4.298685 \n",
      "2019-12-06 16:19:39.516896: step 2290, loss = 5.115566 \n",
      "2019-12-06 16:19:40.303313: step 2300, loss = 3.632863 \n",
      "INFO:tensorflow:Restoring parameters from logs5/train/latest.ckpt\n",
      "Validation accuracy is= 0.213942, best accuracy 0.146334\n",
      "Save file to: logs5/train/model.ckpt-2300\n",
      "2019-12-06 16:19:49.680926: step 2310, loss = 4.125484 \n",
      "2019-12-06 16:19:50.495287: step 2320, loss = 4.393099 \n",
      "2019-12-06 16:19:51.302974: step 2330, loss = 4.045396 \n",
      "2019-12-06 16:19:52.121477: step 2340, loss = 4.054384 \n",
      "2019-12-06 16:19:52.921864: step 2350, loss = 4.256090 \n",
      "2019-12-06 16:19:53.710625: step 2360, loss = 4.407962 \n",
      "2019-12-06 16:19:54.511520: step 2370, loss = 3.757081 \n",
      "2019-12-06 16:19:55.335632: step 2380, loss = 3.129846 \n",
      "2019-12-06 16:19:56.153832: step 2390, loss = 3.908718 \n",
      "2019-12-06 16:19:56.963274: step 2400, loss = 5.244668 \n",
      "INFO:tensorflow:Restoring parameters from logs5/train/latest.ckpt\n",
      "Validation accuracy is= 0.250300, best accuracy 0.213942\n",
      "Save file to: logs5/train/model.ckpt-2400\n",
      "2019-12-06 16:20:06.015832: step 2410, loss = 4.187471 \n",
      "2019-12-06 16:20:06.788947: step 2420, loss = 4.093439 \n",
      "2019-12-06 16:20:07.567878: step 2430, loss = 3.393371 \n",
      "2019-12-06 16:20:08.338389: step 2440, loss = 3.436763 \n",
      "2019-12-06 16:20:09.119781: step 2450, loss = 4.644444 \n",
      "2019-12-06 16:20:09.908635: step 2460, loss = 3.919283 \n",
      "2019-12-06 16:20:10.669878: step 2470, loss = 2.825769 \n",
      "2019-12-06 16:20:11.454693: step 2480, loss = 3.887632 \n",
      "2019-12-06 16:20:12.239359: step 2490, loss = 3.754939 \n",
      "2019-12-06 16:20:13.054983: step 2500, loss = 2.953442 \n",
      "INFO:tensorflow:Restoring parameters from logs5/train/latest.ckpt\n",
      "Validation accuracy is= 0.289663, best accuracy 0.250300\n",
      "Save file to: logs5/train/model.ckpt-2500\n",
      "2019-12-06 16:20:22.679961: step 2510, loss = 2.975834 \n",
      "2019-12-06 16:20:23.471257: step 2520, loss = 3.173447 \n",
      "2019-12-06 16:20:24.266589: step 2530, loss = 3.292250 \n",
      "2019-12-06 16:20:25.064847: step 2540, loss = 4.404703 \n",
      "2019-12-06 16:20:25.853700: step 2550, loss = 3.318416 \n",
      "2019-12-06 16:20:26.648417: step 2560, loss = 4.263123 \n",
      "2019-12-06 16:20:27.441716: step 2570, loss = 4.715496 \n",
      "2019-12-06 16:20:28.240995: step 2580, loss = 3.529576 \n",
      "2019-12-06 16:20:29.036850: step 2590, loss = 4.348540 \n",
      "2019-12-06 16:20:29.839890: step 2600, loss = 3.762814 \n",
      "INFO:tensorflow:Restoring parameters from logs5/train/latest.ckpt\n",
      "INFO:tensorflow:Error reported to Coordinator: <class 'tensorflow.python.framework.errors_impl.CancelledError'>, Enqueue operation was cancelled\n",
      "\t [[{{node batch/fifo_queue_enqueue}}]]\n",
      "INFO:tensorflow:Error reported to Coordinator: <class 'tensorflow.python.framework.errors_impl.CancelledError'>, Enqueue operation was cancelled\n",
      "\t [[{{node input_producer/input_producer_EnqueueMany}}]]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-56d6528abf40>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     23\u001b[0m            \u001b[0mval_tfrecords_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_val_examples\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m            \u001b[0mlog_dir\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m            opt)\n\u001b[0m",
      "\u001b[0;32m<ipython-input-4-bec2aae80163>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(path_to_train_tfrecords_file, num_train_examples, path_to_val_tfrecords_file, num_val_examples, path_to_train_log_dir, training_options)\u001b[0m\n\u001b[1;32m     67\u001b[0m                 accuracy = evaluate(evaluator, path_to_latest_checkpoint_file, path_to_val_tfrecords_file,\n\u001b[1;32m     68\u001b[0m                                               \u001b[0mnum_val_examples\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m                                               global_step_val)\n\u001b[0m\u001b[1;32m     70\u001b[0m                 \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'Validation accuracy is= %f, best accuracy %f'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0maccuracy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbest_accuracy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-24c644db38b0>\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(writer, path_to_checkpoint, path_to_tfrecords_file, num_examples, global_step)\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_batches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m                 \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mupdate_accuracy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m             \u001b[0maccuracy_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msummary_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0maccuracy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msummary\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/envTF113/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    927\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    928\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 929\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    930\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    931\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/envTF113/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1150\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1151\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1152\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1153\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1154\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/envTF113/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1326\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1327\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1328\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1329\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1330\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/envTF113/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1332\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1333\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1334\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1335\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1336\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/envTF113/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1317\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1318\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1319\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/envTF113/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1405\u001b[0m     return tf_session.TF_SessionRun_wrapper(\n\u001b[1;32m   1406\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1407\u001b[0;31m         run_metadata)\n\u001b[0m\u001b[1;32m   1408\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1409\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#Train the model \n",
    "#define the folder path\n",
    "train_tfrecords_file = 'data/train.tfrecords'\n",
    "val_tfrecords_file = 'data/val.tfrecords'\n",
    "tfrecords_meta_file = 'data/meta.json'\n",
    "log_dir = 'logs5/train'\n",
    "opt = {\n",
    "    'batch_size': 32,\n",
    "    'learning_rate': 1e-2,\n",
    "    'patience': 100,\n",
    "    'decay_steps': 10000,\n",
    "    'decay_rate': 0.9\n",
    "    }\n",
    "with open(tfrecords_meta_file, 'r') as f:\n",
    "    content = json.load(f)\n",
    "    num_train_examples = content['num_examples']['train']\n",
    "    num_val_examples = content['num_examples']['val']\n",
    "    num_test_examples = content['num_examples']['test']\n",
    "    \n",
    "    \n",
    "#train the model     \n",
    "train(train_tfrecords_file, num_train_examples,\n",
    "           val_tfrecords_file, num_val_examples,\n",
    "           log_dir, \n",
    "           opt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
